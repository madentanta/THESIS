{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48d0c8ca-2d54-46b0-8e4b-84fd7638abd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Load the Excel file (header starts at row 2)\n",
    "file_path = \"Dataset_PlantAdvisor.xlsx\"\n",
    "df = pd.read_excel(file_path, header=1)\n",
    "\n",
    "# Show current columns\n",
    "print(\"‚úÖ Columns detected:\", df.columns.tolist())\n",
    "\n",
    "# Fill missing Latitude & Longitude\n",
    "if 'Longitude' in df.columns and 'Latitude' in df.columns:\n",
    "    df['Longitude'] = df['Longitude'].fillna(method='ffill').fillna(method='bfill')\n",
    "    df['Latitude'] = df['Latitude'].fillna(method='ffill').fillna(method='bfill')\n",
    "else:\n",
    "    print(\"‚ö†Ô∏è Column names not found ‚Äî please check manually\")\n",
    "\n",
    "# ü™¥ Add new 'Tanaman' column with several plant types\n",
    "# Example plant list ‚Äî you can change or extend this\n",
    "plant_types = [\"Padi\", \"Jagung\", \"Kedelai\", \"Kopi\", \"Tebu\", \"Cabai\", \"Tomat\"]\n",
    "\n",
    "# Randomly assign one plant per row\n",
    "np.random.seed(42)  # for reproducibility\n",
    "df[\"Tanaman\"] = np.random.choice(plant_types, size=len(df))\n",
    "\n",
    "# Save updated dataset to Excel and CSV\n",
    "excel_out = \"Dataset_PlantAdvisor_Filled.xlsx\"\n",
    "csv_out = \"Dataset_PlantAdvisor_Filled.csv\"\n",
    "df.to_excel(excel_out, index=False)\n",
    "df.to_csv(csv_out, index=False)\n",
    "\n",
    "print(\"‚úÖ Dataset updated and saved successfully!\")\n",
    "print(f\"üìÑ Excel: {excel_out}\")\n",
    "print(f\"üìä CSV:   {csv_out}\")\n",
    "\n",
    "# Display a small preview\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4d0966a-6b1b-4df0-a2b0-60bbe98167c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "csv_path = \"Dataset_PlantAdvisor_Filled.csv\"\n",
    "df = pd.read_csv(csv_path)\n",
    "df = df.drop(columns=[\"Nama Daerah\", \"Evidence\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85b80107-8ded-4fe5-95b5-c0061abfde59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "cat_cols = df.select_dtypes(include=[\"object\", \"category\"]).columns.tolist()\n",
    "print(\"Categorical columns:\", cat_cols)\n",
    "encoders = {}\n",
    "\n",
    "for col in cat_cols:\n",
    "    le = LabelEncoder()\n",
    "    df[col] = le.fit_transform(df[col].astype(str))  # cast to str just in case\n",
    "    encoders[col] = le"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d0ab605-a994-4b19-8b29-d74c5c8db96a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(\"data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7de705f5-6871-4a70-b3c2-184a1159c317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing the exact training scenario that causes tensor error...\n",
      "Loaded dataset: 16 samples, 8 features\n",
      "Task: Classification with 5 classes\n",
      "Class distribution: {np.int64(0): np.int64(1), np.int64(1): np.int64(5), np.int64(2): np.int64(2), np.int64(3): np.int64(4), np.int64(4): np.int64(4)}\n",
      "Warning: Removing 1 class(es) with < 2 samples\n",
      "Removed classes: [0]\n",
      "After filtering: 15 samples\n",
      "Using stratified split (min class size: 2)\n",
      "Attempting to run a full training epoch...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 87.11it/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 386.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/100\n",
      "  Train - Loss: 2.0384, Feature Loss: 2.0384, Target Loss: 0.0000\n",
      "  Train - Target Acc: 0.00%\n",
      "  Val - Loss: 0.0000, Acc: 0.00%\n",
      "  LR: 0.001000, Annealing: 1.000\n",
      "  ‚Üí Model saved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 54.56it/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 198.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 2/100\n",
      "  Train - Loss: 1.6963, Feature Loss: 1.6963, Target Loss: 0.0000\n",
      "  Train - Target Acc: 0.00%\n",
      "  Val - Loss: 0.0000, Acc: 0.00%\n",
      "  LR: 0.000999, Annealing: 1.000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 55.73it/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 345.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 3/100\n",
      "  Train - Loss: 1.5971, Feature Loss: 1.5971, Target Loss: 0.0000\n",
      "  Train - Target Acc: 0.00%\n",
      "  Val - Loss: 0.0000, Acc: 0.00%\n",
      "  LR: 0.000998, Annealing: 0.999\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 127.67it/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 598.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 4/100\n",
      "  Train - Loss: 2.1269, Feature Loss: 2.1269, Target Loss: 0.0000\n",
      "  Train - Target Acc: 0.00%\n",
      "  Val - Loss: 0.0000, Acc: 0.00%\n",
      "  LR: 0.000996, Annealing: 0.998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 85.54it/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 384.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 5/100\n",
      "  Train - Loss: 1.4732, Feature Loss: 1.4732, Target Loss: 0.0000\n",
      "  Train - Target Acc: 0.00%\n",
      "  Val - Loss: 0.0000, Acc: 0.00%\n",
      "  LR: 0.000994, Annealing: 0.996\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 112.42it/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 311.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 6/100\n",
      "  Train - Loss: 0.8775, Feature Loss: 0.8775, Target Loss: 0.0000\n",
      "  Train - Target Acc: 0.00%\n",
      "  Val - Loss: 0.0000, Acc: 0.00%\n",
      "  LR: 0.000991, Annealing: 0.994\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 103.20it/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 393.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 7/100\n",
      "  Train - Loss: 0.9613, Feature Loss: 0.9613, Target Loss: 0.0000\n",
      "  Train - Target Acc: 0.00%\n",
      "  Val - Loss: 0.0000, Acc: 0.00%\n",
      "  LR: 0.000988, Annealing: 0.991\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 119.31it/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 291.96it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 8/100\n",
      "  Train - Loss: 1.2012, Feature Loss: 1.2012, Target Loss: 0.0000\n",
      "  Train - Target Acc: 0.00%\n",
      "  Val - Loss: 0.0000, Acc: 0.00%\n",
      "  LR: 0.000984, Annealing: 0.988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 113.98it/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 352.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 9/100\n",
      "  Train - Loss: 1.6514, Feature Loss: 1.6514, Target Loss: 0.0000\n",
      "  Train - Target Acc: 0.00%\n",
      "  Val - Loss: 0.0000, Acc: 0.00%\n",
      "  LR: 0.000980, Annealing: 0.984\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 116.04it/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 418.68it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 10/100\n",
      "  Train - Loss: 1.9661, Feature Loss: 1.9661, Target Loss: 0.0000\n",
      "  Train - Target Acc: 0.00%\n",
      "  Val - Loss: 0.0000, Acc: 0.00%\n",
      "  LR: 0.000976, Annealing: 0.980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 97.66it/s]\n",
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 486.92it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 11/100\n",
      "  Train - Loss: 3.1209, Feature Loss: 3.1209, Target Loss: 0.0000\n",
      "  Train - Target Acc: 0.00%\n",
      "  Val - Loss: 0.0000, Acc: 0.00%\n",
      "  LR: 0.000970, Annealing: 0.976\n",
      "\n",
      "Early stopping at epoch 11\n",
      "\n",
      "Training complete! Best model loaded.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 1/1 [00:00<00:00, 220.39it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 0.0000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from customhopular import load_and_preprocess_csv, Hopular\n",
    "from trainer import create_data_loaders, HopularTrainer\n",
    "\n",
    "\n",
    "# Load and preprocess your CSV\n",
    "X_train, X_val, X_test, y_train, y_val, y_test, metadata = load_and_preprocess_csv(\n",
    "    csv_path=\"data.csv\",\n",
    "    target_column='Tanaman',\n",
    "    categorical_columns=None,\n",
    "    test_size=0.2,\n",
    "    min_class_samples=2\n",
    ")\n",
    "\n",
    "# Create data loaders\n",
    "train_loader, val_loader, test_loader = create_data_loaders(\n",
    "    X_train, X_val, X_test, y_train, y_val, y_test,\n",
    "    metadata, batch_size=32  # Use original batch size\n",
    ")\n",
    "\n",
    "# Create model\n",
    "model = Hopular(\n",
    "    input_sizes=metadata['input_sizes'],\n",
    "    target_discrete=metadata['target_discrete'],\n",
    "    target_numeric=metadata['target_numeric'],\n",
    "    feature_discrete=metadata['feature_discrete'],\n",
    "    memory=metadata['memory'],\n",
    "    feature_size=32,\n",
    "    hidden_size=16,\n",
    "    hidden_size_factor=1.0,\n",
    "    num_heads=4,\n",
    "    num_blocks=1,\n",
    "    scaling_factor=1.0,\n",
    "    input_dropout=0.2,\n",
    "    lookup_dropout=0.2,\n",
    "    output_dropout=0.2,\n",
    ")\n",
    "\n",
    "# Create trainer\n",
    "trainer = HopularTrainer(\n",
    "    model,\n",
    "    learning_rate=1e-3,\n",
    "    weight_decay=0.01,\n",
    "    initial_feature_loss_weight=1.0,\n",
    "    final_feature_loss_weight=0.0\n",
    ")\n",
    "\n",
    "print(\"Attempting to run a full training epoch...\")\n",
    "\n",
    "# Create optimizer\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "    \n",
    "trainer.fit(train_loader, val_loader, metadata, n_epochs=100, patience=10)\n",
    "\n",
    "# 5. Evaluate\n",
    "test_metrics = trainer.evaluate(test_loader, metadata)\n",
    "print(f\"Test Loss: {test_metrics['loss']:.4f}\")\n",
    "if test_metrics['accuracy']:\n",
    "    print(f\"Test Accuracy: {test_metrics['accuracy']:.2%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aea8d949",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(X_train.shape)\n",
    "print(X_val.shape)\n",
    "print(X_test.shape)\n",
    "import pprint\n",
    "pprint.pprint(metadata)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36032e96-c692-4399-a715-6943fedd409e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import customhopular\n",
    "print(dir(customhopular))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2ed6b16-a39c-4f4a-bac9-ca0755702757",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
